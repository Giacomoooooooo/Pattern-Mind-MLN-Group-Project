# PatternMind — Visual Space Organization (Machine Learning Project)

**Team members:** *(fill in)*  
**Captain student id:** *(fill in)*

---

## [Section 1] Introduction
This project explores visual concepts and semantic structures in a labeled image collection.  
Rather than optimizing pure classification accuracy, our goal is to **analyze and organize the visual space**: discover clusters, relationships between categories, and areas of ambiguity.

---

## [Section 2] Methods

### Dataset
- Folder-structured image dataset (one folder per category).
- Images are loaded, resized, normalized, and analyzed.

### Feature Extraction (representations)
We compare multiple feature spaces:
1. **Deep visual features (primary):** a pretrained CNN (e.g., ResNet-50) used as a fixed feature extractor (penultimate layer).
2. **Classical baseline features:** color histograms (and optionally HOG if desired).
3. *(Optional)* **Autoencoder features** for learned unsupervised representations.

### Dimensionality Reduction (visual mapping)
To visualize and interpret the structure of the feature space:
- PCA (global linear structure)
- t-SNE (local neighborhood structure)
- UMAP (manifold structure, often better global+local balance)

### Clustering (organization of the space)
We apply multiple clustering methods:
- K-Means (spherical clusters)
- Agglomerative clustering (hierarchical structure)
- DBSCAN / HDBSCAN (density-based clusters + outliers)

### Environment / Reproducibility
Use the provided `requirements.txt` (pip) or export your conda environment.
- Python: 3.10+ recommended
- Key libs: torch, torchvision, scikit-learn, umap-learn, matplotlib, pandas, pillow

---

## [Section 3] Experimental Design

### Experiment A — Deep features vs Classical features
**Purpose:** assess how representation choice changes clusterability and class separability.  
**Baselines:** color histograms (and optionally HOG) vs CNN features.  
**Metrics:** Silhouette score; (with labels only for interpretation) ARI/NMI; cluster purity table.

### Experiment B — PCA vs t-SNE vs UMAP (visual mappings)
**Purpose:** evaluate how dimensionality reduction changes interpretability and neighborhood preservation.  
**Baselines:** PCA vs t-SNE vs UMAP.  
**Metrics:** qualitative: class overlap and cluster compactness; quantitative: trustworthiness (optional), kNN label agreement.

### Experiment C — Clustering methods comparison
**Purpose:** compare cluster assignments and outlier detection.  
**Baselines:** K-Means vs Agglomerative vs DBSCAN/HDBSCAN.  
**Metrics:** Silhouette, Davies–Bouldin; ARI/NMI (interpretation); outlier rate and examples.

---

## [Section 4] Results

### Main findings
*(fill in after running experiments)*

### Figures (generated by code)
Placeholders (the notebook generates these in `images/`):

1. **2D embedding plots** (PCA/t-SNE/UMAP) colored by category  
   - `images/embedding_umap.png`
2. **Clustering visualization** (embedding colored by cluster id)  
   - `images/clusters_kmeans.png`
3. **Ambiguity analysis** (confusion-like table between clusters and labels)  
   - `images/cluster_label_heatmap.png`
4. **Nearest-neighbor gallery** (examples of most similar images across classes)  
   - `images/nn_gallery.png`

---

## [Section 5] Conclusions

### Take-away
*(fill in)*

### Open questions & next steps
- Improve representations with domain-specific fine-tuning or self-supervised learning.
- Add robustness checks (augmentations, different backbones).
- Explore hierarchical organization (category → subcategory) if metadata exists.

---

## Repository layout
```
.
├── README.md
├── main.ipynb
├── requirements.txt
└── images/
```
